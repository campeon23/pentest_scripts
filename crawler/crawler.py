#!/usr/bin/env python

import argparse
import requests

'''
        Run the script by providing the target_url and wordlist arguments in the following format: 
        
        python crawler.py --target-url google.com --wordlist ./subdomains-wodlist.txt.
'''


def request(url):
    """
    Function to make a get request to the provided URL.
    """
    try:
        return requests.get('http://' + url)
    except requests.exceptions.ConnectionError as err:
        pass  # if there is a connection error, just skip this URL


def get_subdomains(target_url, wordlist):
    """
    Function to iterate over the wordlist and test each as a potential subdomain.
    If a valid response is received, it's printed as a discovered subdomain.
    """
    with open(wordlist, 'r') as wordlist_file:
        for line in wordlist_file:
            test_url = line.strip('\r\n') + '.' + target_url
            response = request(test_url)
            if response:
                print('[+] Discovered subdomain --> ' + test_url)


def main(target_url, wordlist):
    """
    The main function that runs the program.
    """
    get_subdomains(target_url, wordlist)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Subdomain Finder')
    parser.add_argument("-t", "--target-url", dest="target_url", type=str,
                        required=True, help="The target URL")
    parser.add_argument("-w", "--wordlist", dest="wordlist", type=str,
                        required=True, help="The wordlist file")

    args = parser.parse_args()

    main(args.target_url, args.wordlist)
