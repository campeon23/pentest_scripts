#!/usr/bin/env python
import argparse
import requests
import re
import urllib.parse as urlparse

'''
        Run the script by providing the required arguments in the following format:
        
        python spider.py --target_url "http://target.com".
        
        The script will crawl the target URL and print all the discovered links to the terminal.
'''


class WebCrawler:
    """A simple web crawler that extracts and prints all the links from a given URL."""

    def __init__(self, target_url):
        self.target_url = target_url
        self.target_links = []

    def extract_links_from(self, url):
        """Extract all href links from a URL and return as a list."""
        response = requests.get(url)
        return re.findall('(?:href=")(.*?)"', response.content.decode(errors='ignore'))

    def crawl(self, url=None):
        """Crawl the URL and recursively search for links."""
        if url is None:
            url = self.target_url
        href_links = self.extract_links_from(url)
        for link in href_links:
            link = urlparse.urljoin(url, link)

            # Ignore in-page navigation links
            if '#' in link:
                link = link.split('#')[0]

            if self.target_url in link and link not in self.target_links:
                self.target_links.append(link)
                print(link)
                self.crawl(link)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Web Crawler")
    parser.add_argument("-t", "--target-url", dest="target_url", type=str, required=True,
                        help="Target URL to be crawled")
    args = parser.parse_args()

    crawler = WebCrawler(args.target_url)
    crawler.crawl()
